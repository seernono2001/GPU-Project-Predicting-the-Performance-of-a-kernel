{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**ML Models**"
      ],
      "metadata": {
        "id": "9R2wZn7lnfF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing in Data"
      ],
      "metadata": {
        "id": "IkAmNU0bniJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r GPU-Project-Predicting-the-Performance-of-a-kernel\n"
      ],
      "metadata": {
        "id": "7V6SdW3zfufJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cab3f1-b9aa-4af7-a0db-25a9ce80e509"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'GPU-Project-Predicting-the-Performance-of-a-kernel': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/seernono2001/GPU-Project-Predicting-the-Performance-of-a-kernel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVfTVmfenj2P",
        "outputId": "f8de5e1a-53e9-4734-e627-36e9a39d6ef0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPU-Project-Predicting-the-Performance-of-a-kernel'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 285 (delta 96), reused 44 (delta 44), pack-reused 152 (from 1)\u001b[K\n",
            "Receiving objects: 100% (285/285), 753.10 KiB | 2.88 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make all datasets\n",
        "import pandas\n",
        "#results/addition direcotry\n",
        "add_data_2 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/addition/results_addition_GPU_2.csv')\n",
        "add_data_3 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/addition/results_addition_GPU_3.csv')\n",
        "add_data_4 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/addition/results_addition_GPU_4.csv')\n",
        "add_data_5 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/addition/results_addition_GPU_5.csv')\n",
        "mult_data_2 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/multiplication/results_multiplication_GPU_2.csv')\n",
        "mult_data_3 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/multiplication/results_multiplication_GPU_3.csv')\n",
        "mult_data_4 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/multiplication/results_multiplication_GPU_4.csv')\n",
        "mult_data_5 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/multiplication/results_multiplication_GPU_5.csv')\n",
        "sub_data_2 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/subtraction/results_subtraction_GPU_2.csv')\n",
        "sub_data_3 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/subtraction/results_subtraction_GPU_3.csv')\n",
        "sub_data_4 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/subtraction/results_subtraction_GPU_4.csv')\n",
        "sub_data_5 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/subtraction/results_subtraction_GPU_5.csv')\n",
        "red_data_2 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/reduction/results_reduction_GPU_2.csv')\n",
        "red_data_3 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/reduction/results_reduction_GPU_3.csv')\n",
        "red_data_4 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/reduction/results_reduction_GPU_4.csv')\n",
        "red_data_5 = pandas.read_csv('GPU-Project-Predicting-the-Performance-of-a-kernel/results/reduction/results_reduction_GPU_5.csv')\n",
        "#concatenate the rows\n",
        "add_data = pandas.concat([add_data_2, add_data_3, add_data_4, add_data_5])\n",
        "mult_data = pandas.concat([mult_data_2, mult_data_3, mult_data_4, mult_data_5])\n",
        "sub_data = pandas.concat([sub_data_2, sub_data_3, sub_data_4, sub_data_5])\n",
        "red_data = pandas.concat([red_data_2, red_data_3, red_data_4, red_data_5])\n",
        "\n",
        "#concatenate all data\n",
        "data = pandas.concat([add_data, mult_data, sub_data, red_data])\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuQRjgoFpuA8",
        "outputId": "edf78fb7-5574-4ff2-c342-c3d0b3d3841c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Index    GPU  ProblemSize  ThreadsPerBlock  NumBlocks Alignment  \\\n",
            "0        1  GPU_2         1030              448          4      good   \n",
            "1        2  GPU_2         1035              224         10      good   \n",
            "2        3  GPU_2         1041              512          5      good   \n",
            "3        4  GPU_2         1048               47         33       bad   \n",
            "4        5  GPU_2         1053              640          4      good   \n",
            "..     ...    ...          ...              ...        ...       ...   \n",
            "995    996  GPU_5      4194304              128      60517      good   \n",
            "996    997  GPU_5      4194304              912       6246       bad   \n",
            "997    998  GPU_5      4194304              544       9372      good   \n",
            "998    999  GPU_5      4194304              622       9619       bad   \n",
            "999   1000  GPU_5      4194304               96      59110      good   \n",
            "\n",
            "     Operation  Avg_SingleThreadTime(sec)  Avg_KernelTime(sec)  Avg_Speedup  \n",
            "0     addition                   0.000005             0.000010        0.535  \n",
            "1     addition                   0.000005             0.000010        0.540  \n",
            "2     addition                   0.000006             0.000023        0.275  \n",
            "3     addition                   0.000006             0.000010        0.630  \n",
            "4     addition                   0.000006             0.000010        0.570  \n",
            "..         ...                        ...                  ...          ...  \n",
            "995  reduction                   0.020199             0.000116      169.650  \n",
            "996  reduction                   0.011589             0.000177       65.210  \n",
            "997  reduction                   0.011069             0.000128       86.125  \n",
            "998  reduction                   0.010783             0.000135       79.775  \n",
            "999  reduction                   0.020445             0.000090      221.915  \n",
            "\n",
            "[14760 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make global_mem column in data\n",
        "import pandas as pd\n",
        "data['global_mem'] = pd.Series()\n",
        "data.loc[data['GPU'] == 'GPU_2', 'global_mem'] = 11081664\n",
        "data.loc[data['GPU'] == 'GPU_3', 'global_mem'] = 12339520\n",
        "data.loc[data['GPU'] == 'GPU_4', 'global_mem'] = 12493056\n",
        "data.loc[data['GPU'] == 'GPU_5', 'global_mem'] = 12158272\n",
        "\n",
        "#make clock rate\n",
        "data['clock_rate'] = pd.Series()\n",
        "data.loc[data['GPU'] == 'GPU_2', 'clock_rate'] = 7000000\n",
        "data.loc[data['GPU'] == 'GPU_3', 'clock_rate'] = 850000\n",
        "data.loc[data['GPU'] == 'GPU_4', 'clock_rate'] = 1215500\n",
        "data.loc[data['GPU'] == 'GPU_5', 'clock_rate'] = 10501000\n",
        "\n",
        "#encode good \"Alignment\" as 1, bad as 0\n",
        "\n",
        "data['Alignment'] = data['Alignment'].map({'good': 1, 'bad': 0})\n",
        "\n",
        "#make a copy data_use\n",
        "data_use = data.copy()\n",
        "#drop operation Index Avg_SingleThreadTime(sec) Avg_KernelTime(sec), GPU\n",
        "data_use = data_use.drop(['Index', 'Avg_SingleThreadTime(sec)', 'Avg_KernelTime(sec)', 'GPU'], axis=1)\n",
        "print(data_use)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5eO82T5EA-z",
        "outputId": "f7117299-3f47-4349-8cc6-a32f060f62c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ProblemSize  ThreadsPerBlock  NumBlocks  Alignment  Operation  \\\n",
            "0           1030              448          4          1   addition   \n",
            "1           1035              224         10          1   addition   \n",
            "2           1041              512          5          1   addition   \n",
            "3           1048               47         33          0   addition   \n",
            "4           1053              640          4          1   addition   \n",
            "..           ...              ...        ...        ...        ...   \n",
            "995      4194304              128      60517          1  reduction   \n",
            "996      4194304              912       6246          0  reduction   \n",
            "997      4194304              544       9372          1  reduction   \n",
            "998      4194304              622       9619          0  reduction   \n",
            "999      4194304               96      59110          1  reduction   \n",
            "\n",
            "     Avg_Speedup global_mem clock_rate  \n",
            "0          0.535   11081664    7000000  \n",
            "1          0.540   11081664    7000000  \n",
            "2          0.275   11081664    7000000  \n",
            "3          0.630   11081664    7000000  \n",
            "4          0.570   11081664    7000000  \n",
            "..           ...        ...        ...  \n",
            "995      169.650   12158272   10501000  \n",
            "996       65.210   12158272   10501000  \n",
            "997       86.125   12158272   10501000  \n",
            "998       79.775   12158272   10501000  \n",
            "999      221.915   12158272   10501000  \n",
            "\n",
            "[14760 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Addition Models"
      ],
      "metadata": {
        "id": "vyydPn4cpqDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "add_data = data_use[data_use['Operation'] == 'addition']\n",
        "\n",
        "#scale and split data, drop speedup and GPU\n",
        "X = add_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = add_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YY5pgMzyDjj7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Baseline"
      ],
      "metadata": {
        "id": "3ENzQITbDZut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#print deature importances along with their names\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_cLUUGp7Ly-",
        "outputId": "f5642ad7-a17b-4755-d7a6-b9331d522bf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 778.8515536013501\n",
            "R2: 0.6656454367311482\n",
            "ProblemSize : 33.98066909628359\n",
            "ThreadsPerBlock : -0.28534681099117365\n",
            "NumBlocks : -2.252553967526133\n",
            "Alignment : -0.5255094955195451\n",
            "global_mem : -21.050108618158283\n",
            "clock_rate : -25.432761079201473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Big Problem Sizes)"
      ],
      "metadata": {
        "id": "QQVvVDppJS-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same thing but with problem sizes >1 mil\n",
        "add_data_big = add_data[add_data['ProblemSize'] > 1000000]\n",
        "print(add_data_big)\n",
        "X = add_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = add_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4\n",
        "                                                    )\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGUTfuRwIEy1",
        "outputId": "a9c0cf62-d09e-4438-9da3-4f3d9c760ff1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ProblemSize  ThreadsPerBlock  NumBlocks  Alignment Operation  \\\n",
            "601      1000688              160      11611          1  addition   \n",
            "602      1019100               64      21205          1  addition   \n",
            "603      1043712              498       3536          0  addition   \n",
            "604      1058633              329       3969          0  addition   \n",
            "605      1065969              288       6745          1  addition   \n",
            "..           ...              ...        ...        ...       ...   \n",
            "995      4194304              128      60517          1  addition   \n",
            "996      4194304              912       6246          0  addition   \n",
            "997      4194304              544       9372          1  addition   \n",
            "998      4194304              622       9619          0  addition   \n",
            "999      4194304               96      59110          1  addition   \n",
            "\n",
            "     Avg_Speedup global_mem clock_rate  \n",
            "601       80.435   11081664    7000000  \n",
            "602       88.505   11081664    7000000  \n",
            "603      100.470   11081664    7000000  \n",
            "604       99.215   11081664    7000000  \n",
            "605       98.905   11081664    7000000  \n",
            "..           ...        ...        ...  \n",
            "995       11.085   12158272   10501000  \n",
            "996        6.450   12158272   10501000  \n",
            "997       11.135   12158272   10501000  \n",
            "998        5.935   12158272   10501000  \n",
            "999        6.200   12158272   10501000  \n",
            "\n",
            "[1596 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRWhHEBMJ3oH",
        "outputId": "6e46af04-1aea-4171-e407-fb2e55e25fa2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 417.5267268004712\n",
            "R2: 0.8298458971068601\n",
            "ProblemSize : 7.3770666082990495\n",
            "ThreadsPerBlock : -0.4110046590055032\n",
            "NumBlocks : -3.885573276503107\n",
            "Alignment : -0.40087110962907263\n",
            "global_mem : -40.54874543595289\n",
            "clock_rate : -48.78591955514465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Baseline)"
      ],
      "metadata": {
        "id": "8nPCyMofKD6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use the same add_data\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "#bring back add_data\n",
        "add_data = data_use[data_use['Operation'] == 'addition']\n",
        "X = add_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = add_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "8R5v9ffoKRT3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD0vHioRK4xd",
        "outputId": "c481f381-5483-44de-dfbe-7e56e00825b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 16.896273428196874\n",
            "R2: 0.9927465688463563\n",
            "ProblemSize : 0.5441141828406275\n",
            "ThreadsPerBlock : 0.004110292011501818\n",
            "NumBlocks : 0.0065019125870802285\n",
            "Alignment : 0.0002843189069868049\n",
            "global_mem : 0.10751040579450692\n",
            "clock_rate : 0.3374788878592968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Big Problem Sizes)"
      ],
      "metadata": {
        "id": "eMZsmeLCLJab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring back addition_big\n",
        "add_data_big = add_data[add_data['ProblemSize'] > 1000000]\n",
        "X = add_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = add_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "rYcUp942LOVM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBErAjr8LRuP",
        "outputId": "e000e4f2-4be9-4b1b-cbcb-8ffc8a4accf9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 20.961036085749992\n",
            "R2: 0.9919989298495486\n",
            "ProblemSize : 0.02865780189778522\n",
            "ThreadsPerBlock : 0.006188750051112199\n",
            "NumBlocks : 0.014320822111517396\n",
            "Alignment : 0.0003485886579314527\n",
            "global_mem : 0.2556629480690282\n",
            "clock_rate : 0.6948210892126256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtraction Models"
      ],
      "metadata": {
        "id": "r9G3WUodMij1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sub_data = data_use[data_use['Operation'] == 'subtraction']\n",
        "\n",
        "#scale and split data, drop speedup and GPU\n",
        "X = sub_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = sub_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "jDyA1kZqMlle"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Baseline"
      ],
      "metadata": {
        "id": "EiErv1ZLM_EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same as addition\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuRMlpu2NCZ1",
        "outputId": "b72cba14-f3e3-4ed9-f315-039c4d7778d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1517.6723598666733\n",
            "R2: 0.4386118488112076\n",
            "ProblemSize : 35.17637292767973\n",
            "ThreadsPerBlock : -2.4241295578875333\n",
            "NumBlocks : -3.69689319454392\n",
            "Alignment : -1.6149885565533322\n",
            "global_mem : -10.28828270765382\n",
            "clock_rate : 2.2491825776287957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Big Problem Sizes)"
      ],
      "metadata": {
        "id": "cpc2yjOhNNus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sub_data_big = sub_data[sub_data['ProblemSize'] > 1000000]\n",
        "X = sub_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = sub_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "pwlgbr9RNH1-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#importances same as earlier\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO_Kgi2tNc3H",
        "outputId": "3ebd51cd-6904-4679-8665-7b784addb1e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1399.9831153950684\n",
            "R2: 0.3514978019658589\n",
            "ProblemSize : -0.6336803850200631\n",
            "ThreadsPerBlock : -4.045009264243212\n",
            "NumBlocks : -6.93393165176443\n",
            "Alignment : -0.35355672319880305\n",
            "global_mem : -31.384715150069553\n",
            "clock_rate : -6.728413938494489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests"
      ],
      "metadata": {
        "id": "jFY88j21N4rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring back sub data\n",
        "sub_data = data_use[data_use['Operation'] == 'subtraction']\n",
        "#print sub_data\n",
        "print(sub_data)\n",
        "#print add_dataa\n",
        "print(add_data)\n",
        "#save sub data as csv in direcotry\n",
        "sub_data.to_csv('sub_data.csv', index=False)\n",
        "# save add data\n",
        "add_data.to_csv('add_data.csv', index=False)\n",
        "X = sub_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = sub_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJHFp6v5N7a5",
        "outputId": "e4d655c3-d19d-48a5-c4b9-a2bc8f2ecc86"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ProblemSize  ThreadsPerBlock  NumBlocks  Alignment    Operation  \\\n",
            "0           1030              448          4          1  subtraction   \n",
            "1           1035              224         10          1  subtraction   \n",
            "2           1041              512          5          1  subtraction   \n",
            "3           1048               47         33          0  subtraction   \n",
            "4           1053              640          4          1  subtraction   \n",
            "..           ...              ...        ...        ...          ...   \n",
            "995      4194304              128      60517          1  subtraction   \n",
            "996      4194304              912       6246          0  subtraction   \n",
            "997      4194304              544       9372          1  subtraction   \n",
            "998      4194304              622       9619          0  subtraction   \n",
            "999      4194304               96      59110          1  subtraction   \n",
            "\n",
            "     Avg_Speedup global_mem clock_rate  \n",
            "0          0.480   11081664    7000000  \n",
            "1          0.450   11081664    7000000  \n",
            "2          0.515   11081664    7000000  \n",
            "3          0.345   11081664    7000000  \n",
            "4          0.375   11081664    7000000  \n",
            "..           ...        ...        ...  \n",
            "995       30.660   12158272   10501000  \n",
            "996       50.910   12158272   10501000  \n",
            "997       53.865   12158272   10501000  \n",
            "998       30.435   12158272   10501000  \n",
            "999       73.345   12158272   10501000  \n",
            "\n",
            "[4000 rows x 8 columns]\n",
            "     ProblemSize  ThreadsPerBlock  NumBlocks  Alignment Operation  \\\n",
            "0           1030              448          4          1  addition   \n",
            "1           1035              224         10          1  addition   \n",
            "2           1041              512          5          1  addition   \n",
            "3           1048               47         33          0  addition   \n",
            "4           1053              640          4          1  addition   \n",
            "..           ...              ...        ...        ...       ...   \n",
            "995      4194304              128      60517          1  addition   \n",
            "996      4194304              912       6246          0  addition   \n",
            "997      4194304              544       9372          1  addition   \n",
            "998      4194304              622       9619          0  addition   \n",
            "999      4194304               96      59110          1  addition   \n",
            "\n",
            "     Avg_Speedup global_mem clock_rate  \n",
            "0          0.535   11081664    7000000  \n",
            "1          0.540   11081664    7000000  \n",
            "2          0.275   11081664    7000000  \n",
            "3          0.630   11081664    7000000  \n",
            "4          0.570   11081664    7000000  \n",
            "..           ...        ...        ...  \n",
            "995       11.085   12158272   10501000  \n",
            "996        6.450   12158272   10501000  \n",
            "997       11.135   12158272   10501000  \n",
            "998        5.935   12158272   10501000  \n",
            "999        6.200   12158272   10501000  \n",
            "\n",
            "[4000 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSKD4cwhOHS7",
        "outputId": "e30fb707-b730-467f-daef-25738ba53292"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 353.7833423267525\n",
            "R2: 0.8691352746994384\n",
            "ProblemSize : 0.7686109687873373\n",
            "ThreadsPerBlock : 0.03184865180705601\n",
            "NumBlocks : 0.04775843828158745\n",
            "Alignment : 0.003164231799285892\n",
            "global_mem : 0.09718148059743144\n",
            "clock_rate : 0.05143622872730188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Big Problem Sizes)"
      ],
      "metadata": {
        "id": "Px-SM_H1NxqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring bad subtraction\n",
        "sub_data_big = sub_data[sub_data['ProblemSize'] > 1000000]\n",
        "X = sub_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = sub_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "EshB5rPTN2gl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTbScFdKULYK",
        "outputId": "c733be41-bf5f-4312-a4f1-b726bd1f14e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1134.0312531674836\n",
            "R2: 0.4746924071930737\n",
            "ProblemSize : 0.2016812706740655\n",
            "ThreadsPerBlock : 0.09133478887746209\n",
            "NumBlocks : 0.14044289242410335\n",
            "Alignment : 0.007847011655879484\n",
            "global_mem : 0.48721219996702325\n",
            "clock_rate : 0.07148183640146642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplication Models"
      ],
      "metadata": {
        "id": "PZHLPs0pXN_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same pre processing as subtraction\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "mult_data = data_use[data_use['Operation'] == 'multiplication']\n",
        "#scale and split data, drop gpu speedup etc\n",
        "X = mult_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smDwQY1mXf-2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Baseline"
      ],
      "metadata": {
        "id": "61ULf-VXXwn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVUSds2LX3K5",
        "outputId": "a7362ebd-846f-49f3-9343-c86491622cbd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 464478804081.7929\n",
            "R2: 0.778149355520882\n",
            "ProblemSize : 1401778.261462411\n",
            "ThreadsPerBlock : -60181.558827619825\n",
            "NumBlocks : -80376.19165227647\n",
            "Alignment : 10830.573234393167\n",
            "global_mem : -14128.413141305473\n",
            "clock_rate : -2110.5558478779703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Big Problem Sizes)\n",
        "\n"
      ],
      "metadata": {
        "id": "jXLQxySlYJfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make the big problem set\n",
        "mult_data_big = mult_data[mult_data['ProblemSize'] > 1000]\n",
        "X = mult_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "yDwvG3kRYLtO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7sVyX_vYUl5",
        "outputId": "3f07871b-5797-48df-c85b-0c3ab2061fca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 353467221752.14307\n",
            "R2: 0.8826030062629266\n",
            "ProblemSize : 1583257.2488290817\n",
            "ThreadsPerBlock : -39098.31968268906\n",
            "NumBlocks : -130740.24480253982\n",
            "Alignment : 5262.832055953197\n",
            "global_mem : -48818.35933694681\n",
            "clock_rate : -21366.057677438166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Bigger Problem Sizes)\n"
      ],
      "metadata": {
        "id": "IY0PVPjrY0iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now do when problem size >1500\n",
        "mult_data_big = mult_data[mult_data['ProblemSize'] > 1500]\n",
        "X = mult_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "0W3qskS0Y4SZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbWSAKiGY_X4",
        "outputId": "39047111-ea98-4a12-cd17-8ee59b9c168e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 358277685491.7407\n",
            "R2: 0.793208556285937\n",
            "ProblemSize : 1195792.408150924\n",
            "ThreadsPerBlock : -69282.3345051613\n",
            "NumBlocks : -11834.298492982212\n",
            "Alignment : -48006.94403382637\n",
            "global_mem : 26421.64420857969\n",
            "clock_rate : -38462.94584261408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests"
      ],
      "metadata": {
        "id": "aZeCTWiVaVTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bring back mult\n",
        "mult_data = data_use[data_use['Operation'] == 'multiplication']\n",
        "X = mult_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "mlNScpt3ahAh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-DaLaAalvV",
        "outputId": "72415c4b-a803-4c2b-abb1-6d4e839588b1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 36432659533.21756\n",
            "R2: 0.9825985407159521\n",
            "ProblemSize : 0.9744286474954827\n",
            "ThreadsPerBlock : 0.005520513641918728\n",
            "NumBlocks : 0.003771942028837877\n",
            "Alignment : 0.0005754586683942401\n",
            "global_mem : 0.01117844101394296\n",
            "clock_rate : 0.004524997151423358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Big Problem Sizes)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "U-SpBaSaaa52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make bigger\n",
        "mult_data_big = mult_data[mult_data['ProblemSize'] > 1000]\n",
        "X = mult_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Mf0nRNO4atn_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMpR77jwaxbc",
        "outputId": "ea92f8de-cfeb-4047-9836-6637431410ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 138761335397.81378\n",
            "R2: 0.9539132269694076\n",
            "ProblemSize : 0.9345134397528024\n",
            "ThreadsPerBlock : 0.013138003791496404\n",
            "NumBlocks : 0.010133300271573614\n",
            "Alignment : 0.0015385327678500338\n",
            "global_mem : 0.029757390222994173\n",
            "clock_rate : 0.010919333193283288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Bigger Problem Sizes)"
      ],
      "metadata": {
        "id": "vi8nNIhkaeam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bigger problem size\n",
        "mult_data_big = mult_data[mult_data['ProblemSize'] > 1500]\n",
        "X = mult_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = mult_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "cduX9BtFa5OW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FCzP3MQbFTe",
        "outputId": "364b031a-37ae-4d8e-8d09-57252948d71d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 208731169536.9069\n",
            "R2: 0.8795241187365076\n",
            "ProblemSize : 0.8299149313899342\n",
            "ThreadsPerBlock : 0.021038468796463677\n",
            "NumBlocks : 0.020278489765768463\n",
            "Alignment : 0.0038700813933096982\n",
            "global_mem : 0.09682920805077906\n",
            "clock_rate : 0.028068820603744816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimum Reduction Models"
      ],
      "metadata": {
        "id": "jk6hmdOCbcwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reduction data\n",
        "red_data = data_use[data_use['Operation'] == 'reduction']\n",
        "#scale and split data, drop\n",
        "X = red_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = red_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "UH4Q8l8AbhjC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Baseline"
      ],
      "metadata": {
        "id": "Ma3THrPfcG73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lswP9Q8bxIx",
        "outputId": "8d2561ac-4060-4b6e-dc8e-4d7fb059b7bf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1015.9685990868899\n",
            "R2: 0.5647246619964523\n",
            "ProblemSize : 28.168536594602468\n",
            "ThreadsPerBlock : -4.368985559562071\n",
            "NumBlocks : 3.8265942148862404\n",
            "Alignment : -0.47279035375779255\n",
            "global_mem : 0.8446447597899419\n",
            "clock_rate : 16.13196940284343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Big Problem Size)"
      ],
      "metadata": {
        "id": "ecnHnTMndSsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "red_data_big = red_data[red_data['ProblemSize'] > 1000000]\n",
        "X = red_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = red_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "ARItySkDdYGy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEpuj2Y_eED8",
        "outputId": "805b239e-b18e-4761-a8b7-362c55680523"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1490.0181005681047\n",
            "R2: 0.44170925874263056\n",
            "ProblemSize : 6.699676955934169\n",
            "ThreadsPerBlock : -14.207920338461182\n",
            "NumBlocks : -3.1717032196170987\n",
            "Alignment : 0.423091586387004\n",
            "global_mem : 2.2297389464992614\n",
            "clock_rate : 34.480244359814755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests\n"
      ],
      "metadata": {
        "id": "_Yg-CuspiSeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring back red\n",
        "red_data = data_use[data_use['Operation'] == 'reduction']\n",
        "X = red_data.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = red_data['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "ZqUNrAtciUxl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        " print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yubVrWbniZK8",
        "outputId": "73fd5451-e3be-4833-90ed-33b1dffb73ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 803.0590991119218\n",
            "R2: 0.6559422986921752\n",
            "ProblemSize : 0.5528104534985607\n",
            "ThreadsPerBlock : 0.08521419350078596\n",
            "NumBlocks : 0.13407752281737867\n",
            "Alignment : 0.008372701847559196\n",
            "global_mem : 0.12317643553575955\n",
            "clock_rate : 0.096348692799956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Big Problem Sizes)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OZ-zyz6giGB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make bigger than 1 mil\n",
        "red_data_big = red_data[red_data['ProblemSize'] > 1000000]\n",
        "X = red_data_big.drop(['Avg_Speedup','Operation'], axis=1)\n",
        "y = red_data_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8YVrzeDzinKU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        " print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4loD27niztC",
        "outputId": "ce0ce108-7a7c-41a7-f062-f1d28cd1c5f0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2255.4784772036414\n",
            "R2: 0.15490103747869954\n",
            "ProblemSize : 0.07768211769014327\n",
            "ThreadsPerBlock : 0.23049018759918194\n",
            "NumBlocks : 0.2164019295596524\n",
            "Alignment : 0.018767565942656692\n",
            "global_mem : 0.2264903461945223\n",
            "clock_rate : 0.23016785301384346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Everything Model"
      ],
      "metadata": {
        "id": "8SjrcHhJjCHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n"
      ],
      "metadata": {
        "id": "ufpW7fiXjG8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use all data\n",
        "#one hot encode the operation\n",
        "data_use = pd.get_dummies(data_use, columns=['Operation'])\n",
        "#scale and split data\n",
        "X = data_use.drop(['Avg_Speedup'], axis=1)\n",
        "y = data_use['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Kgb_LHr6jEEC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m19kFO91jMJ-",
        "outputId": "cd298329-345a-4e00-9440-389443e5a97c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 440374887791.5114\n",
            "R2: 0.23167485085821782\n",
            "ProblemSize : -268.90379233966235\n",
            "ThreadsPerBlock : 640.839057525387\n",
            "NumBlocks : 802.9102511550009\n",
            "Alignment : -6351.3050147822505\n",
            "global_mem : -4815.720390125765\n",
            "clock_rate : -3293.1070274890617\n",
            "Operation_addition : -86773.93002133246\n",
            "Operation_multiplication : 298175.1806026821\n",
            "Operation_reduction : -87795.29559296348\n",
            "Operation_subtraction : -87269.14834874125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression (Big Problem Sizes)\n"
      ],
      "metadata": {
        "id": "10xyCmFZkQ9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract mult rows in data_use\n",
        "mult_data = data_use[data_use['Operation_multiplication'] == 1]\n",
        "#cut rows with problem size less than 1000\n",
        "mult_data_big = mult_data[mult_data['ProblemSize'] > 1000]\n",
        "#extract the rest\n",
        "ro_data = data_use[data_use['Operation_multiplication'] == 0]\n",
        "#cut rows with problem size less than 1 mil\n",
        "ro_data_big = ro_data[ro_data['ProblemSize'] > 1000000]\n",
        "\n",
        "data_use_big = pd.concat([mult_data_big, ro_data_big])\n",
        "#same pre processing\n",
        "X = data_use_big.drop(['Avg_Speedup'], axis=1)\n",
        "y = data_use_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mfEUAowTkZo2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "\n",
        "feature_names = X.columns\n",
        "feature_importances = model.coef_\n",
        "for i in range(len(feature_names)):\n",
        "    print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFimU6zhlyZl",
        "outputId": "d2effe79-8a2d-4ff8-a5c3-1ad5238d60a9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 494766829824.8516\n",
            "R2: 0.6489866791031838\n",
            "ProblemSize : 10339.006377554679\n",
            "ThreadsPerBlock : -27602.51567553915\n",
            "NumBlocks : -23005.790843730036\n",
            "Alignment : 5350.514804286457\n",
            "global_mem : -4935.833849232848\n",
            "clock_rate : 543.2572055532437\n",
            "Operation_addition : -210307.1071950645\n",
            "Operation_multiplication : 787209.883854299\n",
            "Operation_reduction : -211416.75980593267\n",
            "Operation_subtraction : -212140.98085025966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests\n"
      ],
      "metadata": {
        "id": "P_IGRH6mmMXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring back all data\n",
        "X = data_use.drop(['Avg_Speedup'], axis=1)\n",
        "y = data_use['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "sUzQrIuFl5x-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        " print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi8HeWvlmVWo",
        "outputId": "0236caed-22d1-4548-eb28-2fa9086c8700"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 10509528648.413826\n",
            "R2: 0.9816639518054779\n",
            "ProblemSize : 0.7353562641140766\n",
            "ThreadsPerBlock : 0.004178647814323471\n",
            "NumBlocks : 0.002639448691397007\n",
            "Alignment : 0.0004017581681519581\n",
            "global_mem : 0.008563684031257962\n",
            "clock_rate : 0.003309550991836807\n",
            "Operation_addition : 3.2325792500210444e-10\n",
            "Operation_multiplication : 0.24555064558819917\n",
            "Operation_reduction : 2.3885727581288853e-10\n",
            "Operation_subtraction : 3.8641892446641705e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (Bigger Problem Size)"
      ],
      "metadata": {
        "id": "GoDqZ6T3nNS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bring back\n",
        "X = data_use_big.drop(['Avg_Speedup'], axis=1)\n",
        "y = data_use_big['Avg_Speedup']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "u6zwNN3dnR46"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('MSE:', mse)\n",
        "print('R2:', r2)\n",
        "#importances\n",
        "feature_names = X.columns\n",
        "feature_importances = model.feature_importances_\n",
        "for i in range(len(feature_names)):\n",
        " print(feature_names[i], ':', feature_importances[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JYxxavynaV8",
        "outputId": "54a726f6-c8e8-46e2-95a3-dc490f6342f0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 20945794833.48759\n",
            "R2: 0.9851399637968281\n",
            "ProblemSize : 0.39328115187964596\n",
            "ThreadsPerBlock : 0.0040457481483989885\n",
            "NumBlocks : 0.5159218587783743\n",
            "Alignment : 0.0010819326784533023\n",
            "global_mem : 0.008575348124893106\n",
            "clock_rate : 0.0031870273094236085\n",
            "Operation_addition : 7.092961992035045e-11\n",
            "Operation_multiplication : 0.0739069327131538\n",
            "Operation_reduction : 2.5664260135174496e-10\n",
            "Operation_subtraction : 4.008467968830749e-11\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
